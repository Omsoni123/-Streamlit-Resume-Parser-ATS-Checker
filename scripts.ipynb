{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Step 1: Resume Parsing (Extract Text from PDFs)\n",
    "\n",
    "# We will extract text from PDF resumes using (PyMuPDF (fitz) ), which is lightweight and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdfminer.six) (44.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.5 MB 435.7 kB/s eta 0:00:38\n",
      "   ---------------------------------------- 0.1/16.5 MB 550.5 kB/s eta 0:00:30\n",
      "   ---------------------------------------- 0.1/16.5 MB 731.4 kB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.2/16.5 MB 985.7 kB/s eta 0:00:17\n",
      "    --------------------------------------- 0.3/16.5 MB 1.1 MB/s eta 0:00:15\n",
      "    --------------------------------------- 0.3/16.5 MB 1.2 MB/s eta 0:00:14\n",
      "    --------------------------------------- 0.4/16.5 MB 1.2 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.6/16.5 MB 1.4 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.7/16.5 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.8/16.5 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 0.9/16.5 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.0/16.5 MB 1.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 1.1/16.5 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/16.5 MB 1.9 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.4/16.5 MB 1.9 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.4/16.5 MB 1.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.5/16.5 MB 1.9 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.6/16.5 MB 1.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.8/16.5 MB 2.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.9/16.5 MB 2.0 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.0/16.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 2.2/16.5 MB 2.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.3/16.5 MB 2.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 2.4/16.5 MB 2.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.5/16.5 MB 2.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.6/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.7/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 2.8/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.0/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.1/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.2/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 3.3/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.4/16.5 MB 2.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.6/16.5 MB 2.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 3.7/16.5 MB 2.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 3.9/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 4.0/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 4.1/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.2/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.4/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.5/16.5 MB 2.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 4.6/16.5 MB 2.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 4.8/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 4.9/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.1/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.2/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 5.3/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.4/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.6/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 5.7/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 5.9/16.5 MB 2.4 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 6.0/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 6.2/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 6.3/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 6.5/16.5 MB 2.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 6.6/16.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.8/16.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.9/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.1/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.2/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 7.4/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.5/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.7/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 7.8/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.9/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 7.9/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 8.1/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.3/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.3/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.3/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.6/16.5 MB 2.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 8.8/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.0/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 9.1/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.2/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 9.4/16.5 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.6/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.8/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 9.9/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.0/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.1/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 10.2/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.4/16.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.5/16.5 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.7/16.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 10.8/16.5 MB 2.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 11.0/16.5 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.2/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.3/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.5/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.6/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.8/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.9/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.2/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 12.3/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.5/16.5 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.7/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 12.8/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.0/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.1/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 13.2/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.4/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.5/16.5 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.6/16.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.8/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 14.0/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.1/16.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.3/16.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.5/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.7/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.9/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.0/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.2/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.2/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.5/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.6/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.8/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.9/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 16.1/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.2/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.5/16.5 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.5/16.5 MB 3.1 MB/s eta 0:00:00\n",
      "Using cached pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "Installing collected packages: pymupdf, pdfminer.six\n",
      "Successfully installed pdfminer.six-20240706 pymupdf-1.25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf pdfminer.six pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Code For Parsinh the pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # from pymupdf    \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracing_the_text_from_pdf(file_path):\n",
    "    doc=fitz.open(file_path)\n",
    "    text=''\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=(\"Om_Soni_resume (2) (1).docx\")\n",
    "extracted_data=extracing_the_text_from_pdf(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Om Soni Mob: +91 7304658858 | Email: om.soni2706@gmail.com\n",
      "| GitHub: https://github.com/Omsoni123 | LinkedIn: linkedin.com/\n",
      "in/om-soni-8b0643231 | Portfolio: https://omsoni123.github.io/\n",
      "portfolis/#about\n",
      "Professional Summary\n",
      "Data-driven AI & Data Analyst with expertise in Python, SQL, and\n",
      "machine learning. Proficient in AI model deployment, predictive\n",
      "analytics, cloud computing, and big data engineering. Skilled in\n",
      "building scalable data pipelines, optimizing model performance,\n",
      "and delivering actionable insights to drive business decisions.\n",
      "Passionate about leveraging AI for automation and efficiency.\n",
      "Technical Skills\n",
      "Programming & Analytics: Python, SQL, R, Pandas, NumPy, Scikit-\n",
      "learn, TensorFlow, PyTorch, OpenAI API\n",
      "Data Engineering & ETL: SQL, NoSQL (MongoDB), Apache Spark,\n",
      "Airflow, Data Cleaning, Feature Engineering\n",
      "Data Visualization & BI: Tableau, Power BI, Matplotlib, Seaborn,\n",
      "Looker, Google Data Studio\n",
      "AI & Machine Learning: Deep Learning, NLP, Computer Vision,\n",
      "Time-Series Forecasting, Model Optimization\n",
      "Cloud & Tools: AWS (S3, Lambda, EC2), Google Cloud, Docker, Git,\n",
      "Jupyter Notebooks, Kubernetes\n",
      "Education\n",
      "M.Sc. Artificial Intelligence | KES Shroff College | Expected July\n",
      "2026\n",
      "Coursework: Neural Networks, Deep Learning, Computer Vision,\n",
      "Research Methodologies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " # pip install SpaCy    # important library to parse name and impoartant info \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nðŸ”¹ Extracts name using SpaCyâ€™s named entity recognition (NER).\\nðŸ”¹ Extracts email and phone numbers using regex.\\nðŸ”¹ Extracts skills by matching predefined skills.\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ðŸ”¹ Extracts name using SpaCyâ€™s named entity recognition (NER).\n",
    "ðŸ”¹ Extracts email and phone numbers using regex.\n",
    "ðŸ”¹ Extracts skills by matching predefined skills.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Step 2: Extract Structured Information (NLP Processing)\n",
    "\n",
    "# Now that we have raw text from resumes, weâ€™ll use SpaCy & regex to extract:\n",
    "\n",
    "#nl model for etracts name using SpaCyâ€™s named entity recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# âœ… Name\n",
    "def extracting_name(text):\n",
    "    name_pattern = r\"(?i)(?:name[:\\s]*)?([A-Z][a-z]+\\s[A-Z][a-z]+)\"\n",
    "    matches=re.findall(name_pattern,text)\n",
    "    return matches[0] if matches else \"name not found\"\n",
    "\n",
    "\n",
    "# âœ… Email\n",
    "def extracting_email(text):\n",
    "     email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "     matches= re.findall(email_pattern,text)\n",
    "     return matches[0] if matches else \"email not found\"\n",
    "\n",
    "\n",
    "# âœ… Phone Number\n",
    "def extracting_phone_number (text):\n",
    "    phone_pattern = r\"\\(?\\b\\d{3}[-.) ]?\\d{3}[-. ]?\\d{4}\\b\"\n",
    "    matches=re.findall(phone_pattern,text)\n",
    "    return matches[0] if matches else \"phone number not found\"\n",
    "\n",
    "# âœ… Skills\n",
    "def extracing_skilss(text):\n",
    "    skils_list=[\"MAchine_learing\",\"python\",'pandas','sql',\"ai\"]\n",
    "    skils_found = [skils for skils in skils_list if skils.lower()  in text.lower()]\n",
    "    return skils_found if skils_found else \"now matching skills found\"\n",
    "# âœ… Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Extracted Information:\n",
      "Name : Om Soni\n",
      "email:  om.soni2706@gmail.com\n",
      "phone number : 7304658858\n",
      "skils : ['python', 'pandas', 'sql', 'ai']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"ðŸ“Œ Extracted Information:\")\n",
    "print(\"Name :\", extracting_name(extracted_data))\n",
    "print(\"email: \", extracting_email(extracted_data))\n",
    "print(\"phone number :\", extracting_phone_number(extracted_data) )\n",
    "print(\"skils :\" , extracing_skilss(extracted_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Step 3: Store Extracted Resume Data in MongoDB\n",
    "\n",
    "MongoDB is a NoSQL database, perfect for storing semi-structured resume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pymango\n",
      "  Downloading pymango-0.1.1.tar.gz (2.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.4.3 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymango) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.4.3->pymango) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.4.3->pymango) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.4.3->pymango) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.4.3->pymango) (2023.11.17)\n",
      "Building wheels for collected packages: pymango\n",
      "  Building wheel for pymango (setup.py): started\n",
      "  Building wheel for pymango (setup.py): finished with status 'done'\n",
      "  Created wheel for pymango: filename=pymango-0.1.1-py3-none-any.whl size=4043 sha256=91ca3995f168fb38229b73552677092ad2e7293abeebcada2d58986ac9f43b4b\n",
      "  Stored in directory: c:\\users\\om soni\\appdata\\local\\pip\\cache\\wheels\\f6\\46\\5a\\37f80a146fd6b19216e7317f7a450633359819a0ac21b18a02\n",
      "Successfully built pymango\n",
      "Installing collected packages: pymango\n",
      "Successfully installed pymango-0.1.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_db\"]  # Database name\n",
    "collection = db[\"resumes\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def store_resume_data(pdf_path):\n",
    "    \"\"\"Extracts data and stores it in MongoDB.\"\"\"\n",
    "    text = extracted_data(pdf_path)\n",
    "    \n",
    "    resume_data = {\n",
    "        \"name\": extracting_name(text),\n",
    "        \"email\": extracting_email(text),\n",
    "        \"phone\": extracting_phone_number(text),       \n",
    "        \"skills\": extracing_skilss(text),\n",
    "        \"raw_text\": text  # Storing the full text for reference\n",
    "    }\n",
    "\n",
    "    pdf_path = \"Om_Soni_resume (2) (1).docx\"  # Replace with actual file path\n",
    "    store_resume_data(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local', 'omdb', 'omdb1']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resume stored: {'name': 'Email', 'email': 'om.soni2706@gmail.com', 'phone': '7304658858', 'text': 'Om Soni Mob: +91 7304658858 | Email: om.soni2706@gmail.com\\n| GitHub: https://github.com/Omsoni123 | LinkedIn: linkedin.com/\\nin/om-soni-8b0643231 | Portfolio: https://omsoni123.github.io/\\nportfolis/#about\\nProfessional Summary\\nData-driven AI & Data Analyst with expertise in Python, SQL, and\\nmachine learning. Proficient in AI model deployment, predictive\\nanalytics, cloud computing, and big data engineering. Skilled in\\nbuilding scalable data pipelines, optimizing model performance,\\nand delivering actionable insights to drive business decisions.\\nPassionate about leveraging AI for automation and efficiency.\\nTechnical Skills\\nProgramming & Analytics: Python, SQL, R, Pandas, NumPy, Scikit-\\nlearn, TensorFlow, PyTorch, OpenAI API\\nData Engineering & ETL: SQL, NoSQL (MongoDB), Apache Spark,\\nAirflow, Data Cleaning, Feature Engineering\\nData Visualization & BI: Tableau, Power BI, Matplotlib, Seaborn,\\nLooker, Google Data Studio\\nAI & Machine Learning: Deep Learning, NLP, Computer Vision,\\nTime-Series Forecasting, Model Optimization\\nCloud & Tools: AWS (S3, Lambda, EC2), Google Cloud, Docker, Git,\\nJupyter Notebooks, Kubernetes\\nEducation\\nM.Sc. Artificial Intelligence | KES Shroff College | Expected July\\n2026\\nCoursework: Neural Networks, Deep Learning, Computer Vision,\\nResearch Methodologies\\n B.Sc. Data Science | KES Shroff College | June 2024\\nCGPA: 8.2 | Coursework: Machine Learning, Big Data Analytics,\\nDatabase Management\\nProjects & Experience\\nAI-Powered Chatbot using Pinecone & Sentence Transformers\\nDeveloped an NLP-powered chatbot for real-time query handling.\\nUsed Pinecone for vector storage, Sentence Transformers for\\nembedding, and Lang Chain for seamless processing.\\nBuilt an interactive UI with Stream lit for enhanced user experience.\\nGitHub: https://github.com/Omsoni123/-LLM-Powered-Chatbot\\nCustomer Churn Prediction for E-Commerce\\nDesigned and deployed a predictive ML model using Logistic\\nRegression.\\nApplied SQL for data extraction and Pandas for feature engineering\\nto improve accuracy.\\nDeployed using Flask & Fast API for real-time analysis.\\nGitHub: https://github.com/Omsoni123/Predicting-User-Churn-for-\\nan-E-commerce-Platform/blob/main/main.ipynb\\nStock Price Forecasting with Time-Series Analysis\\nBuilt and optimized ARIMA and LSTM models for financial\\nforecasting.\\nApplied SQL & Pandas for feature selection and preprocessing.\\nAutomated real-time data collection using APIs and web scraping\\ntechniques.\\n Movie Recommendation System\\nBuilt a content-based recommendation engine leveraging Cosine\\nSimilarity.\\nProcessed and cleaned the IMDb dataset using Pandas for better\\npredictions.\\nGitHub: https://github.com/Omsoni123/Movie-Recommendation-\\nSystem-\\nClick Prediction Using Logistic Regression\\nDeveloped an ML model to predict ad-click probability using user\\nbehaviour data.\\nPerformed extensive EDA using Matplotlib and Seaborn for pattern\\nanalysis.\\nEvaluated performance using Confusion Matrix, Accuracy, and\\nPrecision.\\nGitHub: https://github.com/Omsoni123/Logistic-Regression-\\nProject-\\nEmployee Management System\\nEngineered an SQL-based system for managing employees and\\ndepartments.\\nDesigned salary update tracking, ranking functions, and audit logs.\\nOptimized queries for efficiency and scalability.\\nGitHub: https://github.com/Omsoni123/employee-management-\\nsystem-sql/tree/main#code\\nCertifications\\nTensorFlow for Deep Learning (Coursera)\\nMachine Learning Specialization (Stanford University)\\n SQL for Data Science (Great Learning)\\nAWS Cloud Practitioner\\nTableau Desktop Specialist\\nAdvanced Excel (Simplilearn)\\nExtracurricular & Open-Source Contributions\\nKaggle Competitor: Participated in AI and data science hackathons.\\nGitHub Contributor: Published AI projects, automation scripts, and\\ntutorials.\\nAI & ML Workshops: Conducted training sessions on machine\\nlearning and Python development.\\nKey Strengths\\nProficient in AI model development and deployment.\\nStrong problem-solving and data storytelling skills.\\nExpertise in data visualization and business intelligence tools.\\nPassionate about cloud computing, automation, and data-driven\\ndecision-making.\\n', '_id': ObjectId('67bc72cc01bbe2fff9f0026f')}\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF extraction\n",
    "import spacy\n",
    "import pymongo\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load NLP model\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_db\"]\n",
    "collection = db[\"resumes\"]\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    \"\"\"Extract text from PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \" \".join(page.get_text(\"text\") for page in doc)\n",
    "\n",
    "def extract_name(text):\n",
    "    \"\"\"Extract name using SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return \"Not Found\"\n",
    "\n",
    "def extract_email(text):\n",
    "    \"\"\"Extract email using regex.\"\"\"\n",
    "    match = re.search(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    return match.group() if match else \"Not Found\"\n",
    "\n",
    "def extract_phone(text):\n",
    "    \"\"\"Extract phone number using regex.\"\"\"\n",
    "    match = re.search(r\"\\b\\d{10}\\b\", text)\n",
    "    return match.group() if match else \"Not Found\"\n",
    "\n",
    "def store_resume(pdf_path):\n",
    "    \"\"\"Extracts data and stores in MongoDB.\"\"\"\n",
    "    text = extract_text(pdf_path)\n",
    "    data = {\n",
    "        \"name\": extract_name(text),\n",
    "        \"email\": extract_email(text),\n",
    "        \"phone\": extract_phone(text),\n",
    "        \"text\": text\n",
    "    }\n",
    "    collection.insert_one(data)\n",
    "    print(f\"âœ… Resume stored: {data}\")\n",
    "\n",
    "# Test\n",
    "if __name__ == \"__main__\":\n",
    "    store_resume(\"Om_Soni_resume (2) (1).docx\")  # Replace with actual file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:23:23.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.723 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-24 19:23:23.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator(_root_container=1, _parent=DeltaGenerator())"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF\n",
    "import spacy\n",
    "import pymongo\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# MongoDB Connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_db\"]\n",
    "collection = db[\"resumes\"]\n",
    "\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\")\n",
    "    return text\n",
    "\n",
    "# Function to extract name using SpaCy\n",
    "def extract_name(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    return \"Name not found\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"ðŸ“„ AI Resume Screener\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload Resume (PDF)\", type=\"pdf\")\n",
    "\n",
    "if uploaded_file:\n",
    "    st.subheader(\"ðŸ“Œ Extracted Information\")\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text_from_pdf(uploaded_file)\n",
    "    st.write(f\"**Extracted Text:**\\n{text[:500]}...\")  # Display first 500 chars\n",
    "\n",
    "    # Extract name\n",
    "    name = extract_name(text)\n",
    "    st.write(f\"**Candidate Name:** {name}\")\n",
    "\n",
    "    # Save to MongoDB\n",
    "    resume_data = {\"name\": name, \"text\": text}\n",
    "    collection.insert_one(resume_data)\n",
    "    st.success(\"âœ… Resume saved to database!\")\n",
    "\n",
    "st.sidebar.info(\"Upload a PDF resume to extract details.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyterlab in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.9)\n",
      "Requirement already satisfied: streamlit in c:\\users\\om soni\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.39.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement jupyterlab-streamlit (from versions: none)\n",
      "ERROR: No matching distribution found for jupyterlab-streamlit\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install jupyterlab streamlit jupyterlab-streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
